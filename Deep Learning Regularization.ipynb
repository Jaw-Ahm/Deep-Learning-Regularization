{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Regularization\n",
    "We will explore different regularization methods and observe changes to loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Fashion MNIST dataset and Preprocess\n",
    "Store class names under `class_names` and normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = x_train / 255.0\n",
    "x_test_scaled = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Accuracy\n",
    "Using the Keras Sequential model, we will train a model to establish a baseline at 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_base = model.fit(x_train_scaled, y_train, epochs=10,\n",
    "                         validation_data=(x_test_scaled, y_test),verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.3328584358215332\n",
      "\n",
      "Test Accuracy: 0.8846\n"
     ]
    }
   ],
   "source": [
    "test_loss_base, test_acc_base = model.evaluate(x_test_scaled, y_test, verbose=0)\n",
    "print('\\nTest Loss:', test_loss_base)\n",
    "print('\\nTest Accuracy:', test_acc_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Norm Penalties\n",
    "Instead of a simple rectified linear model, we will use a regularized model, in this case we will adjust the L2 norm penalty on weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='elu', \n",
    "                       kernel_regularizer=regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_pnp = model.fit(x_train_scaled, y_train, epochs=10,\n",
    "                        validation_data=(x_test_scaled, y_test),verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.6291851170063019\n",
      "\n",
      "Test Accuracy: 0.8102\n",
      "\n",
      "Test Loss Difference: 0.2963266811847687\n",
      "\n",
      "Test Accuracy Difference: -0.07440001\n"
     ]
    }
   ],
   "source": [
    "test_loss_pnp, test_acc_pnp = model.evaluate(x_test_scaled, y_test, verbose=0)\n",
    "test_acc_diff = test_acc_pnp - test_acc_base\n",
    "test_loss_diff = test_loss_pnp - test_loss_base\n",
    "print('\\nTest Loss:', test_loss_pnp)\n",
    "print('\\nTest Accuracy:', test_acc_pnp)\n",
    "print('\\nTest Loss Difference:', test_loss_diff)\n",
    "print('\\nTest Accuracy Difference:', test_acc_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Accuracy test accuracy went down and loss increased under `l2(0.01)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "Now we will instead try dropout layers which remove a features influence randomly during training to improve robustness to feature loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='elu'),\n",
    "    keras.layers.Dropout(0.05),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_drop = model.fit(x_train_scaled, y_train, epochs=10,\n",
    "                         validation_data=(x_test_scaled, y_test),verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.33070659297704696\n",
      "\n",
      "Test Accuracy: 0.8802\n",
      "\n",
      "Test Loss Difference: -0.002151842844486218\n",
      "\n",
      "Test Accuracy Difference: -0.0043999553\n"
     ]
    }
   ],
   "source": [
    "test_loss_drop, test_acc_drop = model.evaluate(x_test_scaled, y_test, verbose=0)\n",
    "test_acc_diff = test_acc_drop - test_acc_base\n",
    "test_loss_diff = test_loss_drop - test_loss_base\n",
    "print('\\nTest Loss:', test_loss_drop)\n",
    "print('\\nTest Accuracy:', test_acc_drop)\n",
    "print('\\nTest Loss Difference:', test_loss_diff)\n",
    "print('\\nTest Accuracy Difference:', test_acc_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Accuracy and Test loss were slightly lower under `Dropout(0.05)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "Increasing epochs can increase accuracy initially but there is a chance of eventually a decraese in test set accuracy. \n",
    "- We will use `callbacks` set up early stopping. \n",
    "- With `patience=10` the training will halt if accuracy isn't improved within 10 epochs.\n",
    "- With `restore_best_weights=True` the model from the last best epoch will be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = [EarlyStopping(monitor='val_accuracy',patience=5, restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1000\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.5016 - accuracy: 0.8224 - val_loss: 0.4353 - val_accuracy: 0.8465\n",
      "Epoch 2/1000\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.3742 - accuracy: 0.8644 - val_loss: 0.4065 - val_accuracy: 0.8501\n",
      "Epoch 3/1000\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.3375 - accuracy: 0.8764 - val_loss: 0.3631 - val_accuracy: 0.8700\n",
      "Epoch 4/1000\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.3125 - accuracy: 0.8845 - val_loss: 0.3572 - val_accuracy: 0.8692\n",
      "Epoch 5/1000\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2948 - accuracy: 0.8908 - val_loss: 0.3488 - val_accuracy: 0.8763\n",
      "Epoch 6/1000\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.2816 - accuracy: 0.8956 - val_loss: 0.3545 - val_accuracy: 0.8720\n",
      "Epoch 7/1000\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.2691 - accuracy: 0.8995 - val_loss: 0.3323 - val_accuracy: 0.8801\n",
      "Epoch 8/1000\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2568 - accuracy: 0.9042 - val_loss: 0.3297 - val_accuracy: 0.8842\n",
      "Epoch 9/1000\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2468 - accuracy: 0.9082 - val_loss: 0.3326 - val_accuracy: 0.8867\n",
      "Epoch 10/1000\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.2379 - accuracy: 0.9115 - val_loss: 0.3213 - val_accuracy: 0.8878\n",
      "Epoch 11/1000\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.2303 - accuracy: 0.9143 - val_loss: 0.3258 - val_accuracy: 0.8864\n",
      "Epoch 12/1000\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2217 - accuracy: 0.9170 - val_loss: 0.3352 - val_accuracy: 0.8836\n",
      "Epoch 13/1000\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.2147 - accuracy: 0.9189 - val_loss: 0.3336 - val_accuracy: 0.8851\n",
      "Epoch 14/1000\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.2079 - accuracy: 0.9221 - val_loss: 0.3394 - val_accuracy: 0.8853\n",
      "Epoch 15/1000\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2024 - accuracy: 0.9237 - val_loss: 0.3466 - val_accuracy: 0.8868\n"
     ]
    }
   ],
   "source": [
    "history_es = model.fit(x_train_scaled, y_train, epochs=1000,\n",
    "                         validation_data=(x_test_scaled, y_test), \n",
    "                         verbose=1, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.3213296460390091\n",
      "\n",
      "Test Accuracy: 0.8878\n",
      "\n",
      "Test Loss Difference: -0.011528789782524085\n",
      "\n",
      "Test Accuracy Difference: 0.0031999946\n"
     ]
    }
   ],
   "source": [
    "test_loss_es, test_acc_es = model.evaluate(x_test_scaled, y_test, verbose=0)\n",
    "test_acc_diff = test_acc_es - test_acc_base\n",
    "test_loss_diff = test_loss_es - test_loss_base\n",
    "print('\\nTest Loss:', test_loss_es)\n",
    "print('\\nTest Accuracy:', test_acc_es)\n",
    "print('\\nTest Loss Difference:', test_loss_diff)\n",
    "print('\\nTest Accuracy Difference:', test_acc_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 10 Epochs, Test loss decreased and accuracy increased.\n",
    "Even when we set `patience=10` we still only needed 10 epochs to find our best accuracy. May be indication that we are at the limit of our current models capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Robustness\n",
    "Models should be robust to minor variations caused by noise. We can add noise by creating the following function which picks an image at random and adds a random float with mean 0 and standard deviation 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noise(n):\n",
    "    train_noise = np.empty(shape=(0,28,28))\n",
    "    labels_noise = np.empty(shape=(0),dtype=int)\n",
    "    for i in range(n):\n",
    "        a = np.random.normal(loc=0.0, scale=0.01)\n",
    "        b = np.random.randint(0, 6000)\n",
    "        train_noise = np.append(train_noise, np.array([x_train_scaled[b]+a]), axis=0)\n",
    "        labels_noise = np.append(labels_noise, y_train[b])\n",
    "    return train_noise, labels_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_noise, new_labels = create_noise(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled_na = x_train_scaled\n",
    "y_train_na = y_train\n",
    "x_train_scaled_na = np.append(x_train_scaled, new_noise, axis=0)\n",
    "y_train_na = np.append(y_train, new_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_na = model.fit(x_train_scaled_na, y_train_na, epochs=10,\n",
    "                         validation_data=(x_test_scaled, y_test),verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.3394689449429512\n",
      "\n",
      "Test Accuracy: 0.8844\n",
      "\n",
      "Test Loss Difference: 0.006610509121418029\n",
      "\n",
      "Test Accuracy Difference: -0.00019997358\n"
     ]
    }
   ],
   "source": [
    "test_loss_na, test_acc_na = model.evaluate(x_test_scaled, y_test, verbose=0)\n",
    "test_acc_diff = test_acc_na - test_acc_base\n",
    "test_loss_diff = test_loss_na - test_loss_base\n",
    "print('\\nTest Loss:', test_loss_na)\n",
    "print('\\nTest Accuracy:', test_acc_na)\n",
    "print('\\nTest Loss Difference:', test_loss_diff)\n",
    "print('\\nTest Accuracy Difference:', test_acc_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating and training on 4 more training samples with noise added, results in increase to loss and decrease to accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_noise, new_labels = create_noise(1000)\n",
    "x_train_scaled_na = x_train_scaled\n",
    "y_train_na = y_train\n",
    "x_train_scaled_na = np.append(x_train_scaled, new_noise, axis=0)\n",
    "y_train_na = np.append(y_train, new_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_na = model.fit(x_train_scaled_na, y_train_na, epochs=10,\n",
    "                         validation_data=(x_test_scaled, y_test),verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.3686344741135836\n",
      "\n",
      "Test Accuracy: 0.8822\n",
      "\n",
      "Test Loss Difference: 0.035776038292050394\n",
      "\n",
      "Test Accuracy Difference: -0.002399981\n"
     ]
    }
   ],
   "source": [
    "test_loss_na, test_acc_na = model.evaluate(x_test_scaled, y_test, verbose=0)\n",
    "test_acc_diff = test_acc_na - test_acc_base\n",
    "test_loss_diff = test_loss_na - test_loss_base\n",
    "print('\\nTest Loss:', test_loss_na)\n",
    "print('\\nTest Accuracy:', test_acc_na)\n",
    "print('\\nTest Loss Difference:', test_loss_diff)\n",
    "print('\\nTest Accuracy Difference:', test_acc_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating and training on 100 more training samples with noise added, results in increase to loss and decrease to accuracy once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_noise, new_labels = create_noise(10000)\n",
    "x_train_scaled_na = x_train_scaled\n",
    "y_train_na = y_train\n",
    "x_train_scaled_na = np.append(x_train_scaled, new_noise, axis=0)\n",
    "y_train_na = np.append(y_train, new_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_na = model.fit(x_train_scaled_na, y_train_na, epochs=10,\n",
    "                         validation_data=(x_test_scaled, y_test),verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.41340110213160514\n",
      "\n",
      "Test Accuracy: 0.8898\n",
      "\n",
      "Test Loss Difference: 0.08054266631007195\n",
      "\n",
      "Test Accuracy Difference: 0.0052000284\n"
     ]
    }
   ],
   "source": [
    "test_loss_na, test_acc_na = model.evaluate(x_test_scaled, y_test, verbose=0)\n",
    "test_acc_diff = test_acc_na - test_acc_base\n",
    "test_loss_diff = test_loss_na - test_loss_base\n",
    "print('\\nTest Loss:', test_loss_na)\n",
    "print('\\nTest Accuracy:', test_acc_na)\n",
    "print('\\nTest Loss Difference:', test_loss_diff)\n",
    "print('\\nTest Accuracy Difference:', test_acc_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating and training on 1000 more training samples with noise added, results in increase to loss and an increase in accuracy. The current model may just be at its max capability as in all tests, variations in accuracy have been very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Augmentation - Translation\n",
    "Another way to increase the amount of data our models have is through image translation. We will simply shift each image to the left, right, up and down. This results in 240,000 new images along with the original 60,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augx1 = np.roll(x_train_scaled, 1, axis=1)\n",
    "train_augx2 = np.roll(x_train_scaled, -1, axis=1)\n",
    "train_augy1 = np.roll(x_train_scaled, 1, axis=2)\n",
    "train_augy2 = np.roll(x_train_scaled, -1, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = np.append(x_train_scaled, train_augx1, axis=0)\n",
    "train_aug = np.append(train_aug, train_augx2, axis=0)\n",
    "train_aug = np.append(train_aug, train_augy1, axis=0)\n",
    "train_aug = np.append(train_aug, train_augy2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lab = np.append(y_train, y_train, axis=0)\n",
    "train_lab = np.append(train_lab, y_train, axis=0)\n",
    "train_lab = np.append(train_lab, y_train, axis=0)\n",
    "train_lab = np.append(train_lab, y_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make sure our function was correct by looking at an original image, and its 4 translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "k=0\n",
    "for i in range(1455,300000,60000):\n",
    "    k=k+1\n",
    "    plt.subplot(1,5,k)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_aug[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_lab[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "300000/300000 [==============================] - 16s 54us/sample - loss: 0.4337 - accuracy: 0.8429 - val_loss: 0.3591 - val_accuracy: 0.8742\n",
      "Epoch 2/10\n",
      "300000/300000 [==============================] - 16s 54us/sample - loss: 0.3383 - accuracy: 0.8759 - val_loss: 0.3420 - val_accuracy: 0.8761\n",
      "Epoch 3/10\n",
      "300000/300000 [==============================] - 16s 53us/sample - loss: 0.3104 - accuracy: 0.8857 - val_loss: 0.3355 - val_accuracy: 0.8806\n",
      "Epoch 4/10\n",
      "300000/300000 [==============================] - 16s 52us/sample - loss: 0.2927 - accuracy: 0.8921 - val_loss: 0.3211 - val_accuracy: 0.8872\n",
      "Epoch 5/10\n",
      "300000/300000 [==============================] - 16s 52us/sample - loss: 0.2803 - accuracy: 0.8964 - val_loss: 0.3180 - val_accuracy: 0.8882\n",
      "Epoch 6/10\n",
      "300000/300000 [==============================] - 16s 52us/sample - loss: 0.2702 - accuracy: 0.8992 - val_loss: 0.3296 - val_accuracy: 0.8897\n",
      "Epoch 7/10\n",
      "300000/300000 [==============================] - 16s 52us/sample - loss: 0.2629 - accuracy: 0.9016 - val_loss: 0.3252 - val_accuracy: 0.8893\n",
      "Epoch 8/10\n",
      "300000/300000 [==============================] - 16s 52us/sample - loss: 0.2548 - accuracy: 0.9050 - val_loss: 0.3207 - val_accuracy: 0.8938\n",
      "Epoch 9/10\n",
      "300000/300000 [==============================] - 16s 53us/sample - loss: 0.2489 - accuracy: 0.9069 - val_loss: 0.3289 - val_accuracy: 0.8894\n",
      "Epoch 10/10\n",
      "300000/300000 [==============================] - 15s 52us/sample - loss: 0.2431 - accuracy: 0.9089 - val_loss: 0.3237 - val_accuracy: 0.8933\n"
     ]
    }
   ],
   "source": [
    "history_aug = model.fit(train_aug, train_lab, epochs=10,  \n",
    "                         validation_data=(x_test_scaled, y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.3237442334771156\n",
      "\n",
      "Test Accuracy: 0.8933\n",
      "\n",
      "Test Loss Difference: -0.009114202344417566\n",
      "\n",
      "Test Accuracy Difference: 0.008700013\n"
     ]
    }
   ],
   "source": [
    "test_loss_aug, test_acc_aug = model.evaluate(x_test_scaled, y_test, verbose=0)\n",
    "test_acc_diff = test_acc_aug - test_acc_base\n",
    "test_loss_diff = test_loss_aug - test_loss_base\n",
    "print('\\nTest Loss:', test_loss_aug)\n",
    "print('\\nTest Accuracy:', test_acc_aug)\n",
    "print('\\nTest Loss Difference:', test_loss_diff)\n",
    "print('\\nTest Accuracy Difference:', test_acc_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we had very insignificant changes. Loss has decreased from base, and accuracy has increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "For this notebook we have:\n",
    "- Applied parrameter norm penalties\n",
    "- Applied dropout\n",
    "- Applied early stopping\n",
    "- Introduced data samples with noise added\n",
    "- Introduced data samples that are translated or shifted\n",
    "\nThese are tools all useful to help increase accuracy and decrease loss in training models. However with our current model, hyperparameters and dataset these improvements were not fully apparent, and further testing of parameters needs to be done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
